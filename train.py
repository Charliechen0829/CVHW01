import os
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tqdm import tqdm
from dataset import FloorPlanDataset
from model import UNet
from config import config
from torchvision import transforms as T


def get_transforms():
    """è·å–æ•°æ®å¢å¼ºå˜æ¢"""
    train_transform = T.Compose([
        T.Resize((416, 416)),
        T.RandomHorizontalFlip(p=0.5),
        T.RandomVerticalFlip(p=0.5),
        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = T.Compose([
        T.Resize((416, 416)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform


def train_fn(loader, model, optimizer, loss_fn, scaler, device):
    """å•ä¸ªepochçš„è®­ç»ƒå¾ªç¯"""
    model.train()
    total_loss = 0.0
    loop = tqdm(loader, desc="Training", leave=False)

    for batch_idx, (data, targets) in enumerate(loop):
        data = data.to(device)
        targets = targets.to(device)

        # éªŒè¯æ ‡ç­¾åˆæ³•æ€§
        assert targets.min() >= 0 and targets.max() < config.NUM_CLASSES, \
            f"Invalid labels: min={targets.min()}, max={targets.max()}"

        # æ··åˆç²¾åº¦è®­ç»ƒ
        with torch.amp.autocast(device_type='cuda'):
            predictions = model(data)
            loss = loss_fn(predictions, targets)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # æ›´æ–°ç»Ÿè®¡é‡
        total_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    return total_loss / len(loader)


def evaluate_fn(loader, model, loss_fn, device):
    """å•ä¸ªepochçš„éªŒè¯å¾ªç¯"""
    model.eval()
    total_loss = 0.0
    loop = tqdm(loader, desc="Validation", leave=False)

    with torch.no_grad():
        for data, targets in loop:
            data = data.to(device)
            targets = targets.to(device)

            # å‰å‘ä¼ æ’­
            with torch.amp.autocast(device_type='cuda'):
                predictions = model(data)
                loss = loss_fn(predictions, targets)

            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())

    return total_loss / len(loader)


def save_checkpoint(state, filename):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    torch.save(state, filename)
    print(f"\nâœ… Checkpoint saved to {filename}")


def main():
    # åˆå§‹åŒ–é…ç½®
    os.makedirs(config.SAVE_DIR, exist_ok=True)
    train_transform, val_transform = get_transforms()

    # åˆ›å»ºæ•°æ®é›†
    train_ds = FloorPlanDataset(
        image_dir=config.TRAIN_IMAGES,
        annotation_path=config.TRAIN_ANNOTATIONS,
        transform=train_transform
    )
    val_ds = FloorPlanDataset(
        image_dir=config.VAL_IMAGES,
        annotation_path=config.VAL_ANNOTATIONS,
        transform=val_transform
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_ds,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    # åˆå§‹åŒ–æ¨¡å‹
    model = UNet(n_channels=3, n_classes=config.NUM_CLASSES).to(config.DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    loss_fn = nn.CrossEntropyLoss()
    scaler = torch.amp.GradScaler()

    # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
    train_losses = []
    val_losses = []
    best_loss = float('inf')
    best_epoch = -1

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.NUM_EPOCHS):
        print(f"\nEpoch {epoch + 1}/{config.NUM_EPOCHS} {'-' * 30}")

        # è®­ç»ƒ
        train_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler, config.DEVICE)
        train_losses.append(train_loss)

        # éªŒè¯
        val_loss = evaluate_fn(val_loader, model, loss_fn, config.DEVICE)
        val_losses.append(val_loss)

        # æ›´æ–°
        if val_loss < best_loss:
            best_loss = val_loss
            best_epoch = epoch
            checkpoint = {
                "epoch": epoch + 1,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "best_loss": best_loss,
            }
            save_checkpoint(
                checkpoint,
                filename=os.path.join(config.SAVE_DIR, config.MODEL_NAME)
            )

        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # å¯è§†åŒ–ä¿å­˜é€»è¾‘
        if epoch == config.NUM_EPOCHS - 1:
            plot_loss_curves(
                train_losses,
                val_losses,
                best_epoch,
                best_loss,
                save_path=os.path.join(config.SAVE_DIR, f"loss_epoch_{epoch + 1:03d}.png")
            )

    print("\nğŸ† Training completed!")
    print(f"Best validation loss: {best_loss:.4f} at epoch {best_epoch + 1}")


def plot_loss_curves(train_losses, val_losses, best_epoch, best_val_loss, save_path):
    plt.figure(figsize=(12, 8))

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'b-o', linewidth=2, markersize=8, label='Training Loss')
    plt.plot(epochs, val_losses, 'r-s', linewidth=2, markersize=8, label='Validation Loss')

    # æ ‡è®°æœ€ä½³éªŒè¯ç‚¹
    plt.scatter(best_epoch + 1, best_val_loss, s=300, marker='*',
                color='gold', edgecolors='black', zorder=10,
                label=f'Best Val: {best_val_loss:.4f} @ Epoch {best_epoch + 1}')

    plt.title('Training Progress', fontsize=16, pad=20)
    plt.xlabel('Epochs', fontsize=12)
    plt.ylabel('Cross Entropy Loss', fontsize=12)
    plt.xticks(epochs, rotation=45)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(loc='upper right', fontsize=10)

    all_losses = train_losses + val_losses
    plt.ylim(min(all_losses) * 0.9, max(all_losses) * 1.1)

    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()


if __name__ == "__main__":
    main()
